#!/usr/bin/env python
"""
SRGAN Specific Weights Loader
Builds SRGAN architecture that exactly matches your weights file structure
Based on the inspection of your .h5 file structure
"""

import os
import cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, Model
from tensorflow.keras.optimizers import Adam
import h5py
import json
from pathlib import Path

class SRGANSpecificLoader:
    """
    SRGAN loader that matches your exact weights file structure
    
    Based on your file structure:
    - Input layer: input_3
    - Conv layers: conv2d, conv2d_1, ..., conv2d_34
    - Upsampling: conv2d_1_scale_2, conv2d_2_scale_2 (2 upsampling blocks = 4x)
    - Add layers: add, add_1, ..., add_16 (residual connections)
    - Lambda layers: lambda, lambda_1, lambda_2, lambda_3 (probably depth_to_space)
    """
    
    def __init__(self):
        self.input_size = (48, 48, 3)
        self.output_size = (192, 192, 3)
        self.scale_factor = 4
        
    def build_matching_architecture(self):
        """
        Build SRGAN architecture that matches your weights structure exactly
        """
        inputs = layers.Input(shape=self.input_size, name='input_3')
        
        # Initial conv layer - conv2d (3,3,3,64)
        x = layers.Conv2D(64, 3, padding='same', name='conv2d')(inputs)
        x = layers.PReLU(shared_axes=[1, 2])(x)
        skip_connection = x
        
        # Residual blocks (16 blocks based on your add layers add_1 to add_16)
        # Each residual block has 2 conv layers
        for i in range(1, 17):  # add_1 to add_16
            # First conv in residual block
            conv_name_1 = f'conv2d_{i*2-1}'  # conv2d_1, conv2d_3, conv2d_5, ...
            x = layers.Conv2D(64, 3, padding='same', name=conv_name_1)(x)
            x = layers.BatchNormalization()(x)
            x = layers.PReLU(shared_axes=[1, 2])(x)
            
            # Second conv in residual block
            conv_name_2 = f'conv2d_{i*2}'    # conv2d_2, conv2d_4, conv2d_6, ...
            residual_input = x
            x = layers.Conv2D(64, 3, padding='same', name=conv_name_2)(x)
            x = layers.BatchNormalization()(x)
            
            # Add connection (skip connection within residual block)
            x = layers.Add(name=f'add_{i}')([x, residual_input])
        
        # Post-residual conv - this should be conv2d_33
        x = layers.Conv2D(64, 3, padding='same', name='conv2d_33')(x)
        x = layers.BatchNormalization()(x)
        
        # Main skip connection
        x = layers.Add(name='add')([x, skip_connection])
        
        # First upsampling block - conv2d_1_scale_2 (64 -> 256 channels for 2x upsampling)
        x = layers.Conv2D(256, 3, padding='same', name='conv2d_1_scale_2')(x)
        x = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2), name='lambda_1')(x)
        x = layers.PReLU(shared_axes=[1, 2])(x)
        
        # Second upsampling block - conv2d_2_scale_2 (64 -> 256 channels for 2x upsampling)
        x = layers.Conv2D(256, 3, padding='same', name='conv2d_2_scale_2')(x)
        x = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2), name='lambda_2')(x)
        x = layers.PReLU(shared_axes=[1, 2])(x)
        
        # Final output layer - conv2d_34 (64 -> 3 channels)
        outputs = layers.Conv2D(3, 3, activation='tanh', padding='same', name='conv2d_34')(x)
        
        model = Model(inputs, outputs, name='SRGAN_Generator_Exact')
        return model
    
    def build_alternative_architecture(self):
        """
        Alternative architecture if the first one doesn't match exactly
        Based on the pattern I see in your weights
        """
        inputs = layers.Input(shape=self.input_size, name='input_3')
        
        # Initial conv - conv2d
        x = layers.Conv2D(64, 3, padding='same', name='conv2d')(inputs)
        x = layers.PReLU(shared_axes=[1, 2])(x)
        skip_connection = x
        
        # Based on your structure, you have conv2d_1 through conv2d_33
        # This suggests a different pattern - let me try sequential approach
        
        # Residual blocks - simpler approach
        for i in range(1, 33):  # conv2d_1 to conv2d_32
            if f'conv2d_{i}' in ['conv2d_1_scale_2', 'conv2d_2_scale_2']:
                continue  # Skip upsampling layers for now
            
            if i <= 32:  # Regular conv layers
                residual_input = x
                x = layers.Conv2D(64, 3, padding='same', name=f'conv2d_{i}')(x)
                
                # Add batch norm and activation
                if i % 2 == 0:  # Every second layer, add skip connection
                    x = layers.BatchNormalization()(x)
                    if i <= 32:  # Within residual block range
                        x = layers.Add(name=f'add_{i//2}')([x, residual_input])
                else:
                    x = layers.BatchNormalization()(x)
                    x = layers.PReLU(shared_axes=[1, 2])(x)
        
        # Main skip connection
        x = layers.Add(name='add')([x, skip_connection])
        
        # Upsampling blocks
        x = layers.Conv2D(256, 3, padding='same', name='conv2d_1_scale_2')(x)
        x = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2), name='lambda_1')(x)
        x = layers.PReLU(shared_axes=[1, 2])(x)
        
        x = layers.Conv2D(256, 3, padding='same', name='conv2d_2_scale_2')(x)
        x = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2), name='lambda_2')(x)
        x = layers.PReLU(shared_axes=[1, 2])(x)
        
        # Final output
        outputs = layers.Conv2D(3, 3, activation='tanh', padding='same', name='conv2d_34')(x)
        
        model = Model(inputs, outputs, name='SRGAN_Generator_Alt')
        return model
    
    def analyze_weights_structure(self, weights_path):
        """
        Analyze your specific weights file structure
        """
        print("üîç Analyzing your specific weights file structure...")
        
        structure_info = {
            'conv_layers': [],
            'add_layers': [],
            'lambda_layers': [],
            'upsampling_layers': [],
            'input_layer': None,
            'output_layer': None
        }
        
        try:
            with h5py.File(weights_path, 'r') as f:
                def analyze_item(name, obj):
                    if isinstance(obj, h5py.Group):
                        layer_name = name.split('/')[-1]
                        
                        if layer_name.startswith('conv2d'):
                            if 'scale' in layer_name:
                                structure_info['upsampling_layers'].append(layer_name)
                            else:
                                structure_info['conv_layers'].append(layer_name)
                        elif layer_name.startswith('add'):
                            structure_info['add_layers'].append(layer_name)
                        elif layer_name.startswith('lambda'):
                            structure_info['lambda_layers'].append(layer_name)
                        elif layer_name.startswith('input'):
                            structure_info['input_layer'] = layer_name
                
                f.visititems(analyze_item)
                
                # Sort layers numerically
                structure_info['conv_layers'].sort(key=lambda x: int(x.split('_')[1]) if '_' in x and x.split('_')[1].isdigit() else 0)
                structure_info['add_layers'].sort(key=lambda x: int(x.split('_')[1]) if '_' in x and x.split('_')[1].isdigit() else 0)
                
                print(f"üìä Analysis results:")
                print(f"   Input layer: {structure_info['input_layer']}")
                print(f"   Conv layers: {len(structure_info['conv_layers'])} layers")
                print(f"   Add layers: {len(structure_info['add_layers'])} layers")
                print(f"   Lambda layers: {len(structure_info['lambda_layers'])} layers")
                print(f"   Upsampling layers: {structure_info['upsampling_layers']}")
                
                # Identify output layer (highest numbered conv2d)
                if structure_info['conv_layers']:
                    # Find conv2d_34 or highest numbered layer
                    conv_numbers = []
                    for layer in structure_info['conv_layers']:
                        if layer == 'conv2d':
                            conv_numbers.append(0)
                        elif '_' in layer:
                            try:
                                num = int(layer.split('_')[1])
                                conv_numbers.append(num)
                            except:
                                pass
                    
                    if conv_numbers:
                        max_conv = max(conv_numbers)
                        structure_info['output_layer'] = f'conv2d_{max_conv}' if max_conv > 0 else 'conv2d'
                        print(f"   Output layer: {structure_info['output_layer']}")
                
                return structure_info
                
        except Exception as e:
            print(f"‚ùå Error analyzing structure: {e}")
            return None
    
    def load_weights_with_mapping(self, model, weights_path):
        """
        Load weights with careful layer mapping
        """
        print("üîß Loading weights with careful mapping...")
        
        try:
            # First try direct loading
            model.load_weights(weights_path)
            print("‚úÖ Direct weight loading successful!")
            return True, "direct"
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Direct loading failed: {e}")
            
        try:
            # Manual weight loading with mapping
            with h5py.File(weights_path, 'r') as weights_file:
                
                # Get model layers that have weights
                model_layers = {layer.name: layer for layer in model.layers if layer.weights}
                
                print(f"üìä Model has {len(model_layers)} layers with weights")
                
                successful_loads = 0
                failed_loads = 0
                
                for layer_name, layer in model_layers.items():
                    if layer_name in weights_file:
                        try:
                            # Get weights for this layer
                            layer_group = weights_file[layer_name]
                            
                            if layer_name in layer_group:
                                layer_weights_group = layer_group[layer_name]
                                
                                # Load kernel and bias if they exist
                                weights_to_load = []
                                
                                if 'kernel:0' in layer_weights_group:
                                    kernel = layer_weights_group['kernel:0'][:]
                                    weights_to_load.append(kernel)
                                
                                if 'bias:0' in layer_weights_group:
                                    bias = layer_weights_group['bias:0'][:]
                                    weights_to_load.append(bias)
                                
                                if weights_to_load:
                                    # Check if shapes match
                                    layer_weight_shapes = [w.shape for w in layer.weights]
                                    file_weight_shapes = [w.shape for w in weights_to_load]
                                    
                                    if layer_weight_shapes == file_weight_shapes:
                                        layer.set_weights(weights_to_load)
                                        successful_loads += 1
                                        print(f"  ‚úÖ {layer_name}: {file_weight_shapes}")
                                    else:
                                        print(f"  ‚ùå {layer_name}: Shape mismatch - model: {layer_weight_shapes}, file: {file_weight_shapes}")
                                        failed_loads += 1
                                else:
                                    print(f"  ‚ö†Ô∏è  {layer_name}: No weights found in file")
                                    failed_loads += 1
                        except Exception as layer_e:
                            print(f"  ‚ùå {layer_name}: Error loading - {layer_e}")
                            failed_loads += 1
                    else:
                        print(f"  ‚ö†Ô∏è  {layer_name}: Not found in weights file")
                        failed_loads += 1
                
                print(f"\nüìä Weight loading summary:")
                print(f"   ‚úÖ Successful: {successful_loads} layers")
                print(f"   ‚ùå Failed: {failed_loads} layers")
                
                if successful_loads > 0:
                    return True, f"manual ({successful_loads}/{successful_loads + failed_loads})"
                else:
                    return False, "no_weights_loaded"
                    
        except Exception as e:
            print(f"‚ùå Manual loading failed: {e}")
            return False, f"error: {e}"
    
    def create_srgan_from_your_weights(self, weights_path):
        """
        Main function to create SRGAN model from your specific weights file
        """
        print("üèóÔ∏è  Creating SRGAN from your specific weights file")
        print("=" * 60)
        
        # Analyze weights structure
        structure = self.analyze_weights_structure(weights_path)
        
        if structure is None:
            raise ValueError("Could not analyze weights file structure")
        
        # Try to build matching architecture
        print(f"\nüèóÔ∏è  Building architecture to match your weights...")
        
        # Try first architecture
        try:
            print("üìê Attempting primary architecture...")
            model = self.build_matching_architecture()
            
            print(f"‚úÖ Primary architecture built:")
            print(f"   Input: {model.input_shape}")
            print(f"   Output: {model.output_shape}")
            print(f"   Layers: {len(model.layers)}")
            
            # Try to load weights
            success, method = self.load_weights_with_mapping(model, weights_path)
            
            if success:
                print(f"‚úÖ Weights loaded successfully using: {method}")
                return model, method
            else:
                print(f"‚ö†Ô∏è  Primary architecture weight loading failed: {method}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Primary architecture failed: {e}")
        
        # Try alternative architecture
        try:
            print("\nüìê Attempting alternative architecture...")
            model = self.build_alternative_architecture()
            
            print(f"‚úÖ Alternative architecture built:")
            print(f"   Input: {model.input_shape}")
            print(f"   Output: {model.output_shape}")
            print(f"   Layers: {len(model.layers)}")
            
            # Try to load weights
            success, method = self.load_weights_with_mapping(model, weights_path)
            
            if success:
                print(f"‚úÖ Weights loaded successfully using: {method}")
                return model, method
            else:
                print(f"‚ö†Ô∏è  Alternative architecture weight loading failed: {method}")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  Alternative architecture failed: {e}")
        
        # If both fail, return architecture without weights
        print("\n‚ö†Ô∏è  Creating architecture without pretrained weights...")
        model = self.build_matching_architecture()
        return model, "architecture_only"
    
    def test_model_functionality(self, model):
        """
        Test the loaded model with sample data
        """
        print(f"\nüß™ Testing model functionality...")
        
        # Create test input
        test_input = np.random.random((1, 48, 48, 3)).astype(np.float32)
        
        # Test different normalization ranges
        test_ranges = [
            ("0-1 range", test_input),
            ("-1 to 1 range", test_input * 2 - 1),
            ("0-255 range", test_input * 255)
        ]
        
        working_range = None
        
        for range_name, test_data in test_ranges:
            try:
                output = model.predict(test_data, verbose=0)
                
                # Check output validity
                if not np.isnan(output).any() and not np.isinf(output).any():
                    output_min, output_max = output.min(), output.max()
                    print(f"‚úÖ {range_name}: Shape {output.shape}, Range [{output_min:.3f}, {output_max:.3f}]")
                    if working_range is None:
                        working_range = range_name
                else:
                    print(f"‚ùå {range_name}: Contains NaN/Inf values")
                    
            except Exception as e:
                print(f"‚ùå {range_name}: {e}")
        
        return working_range
    
    def save_complete_model(self, model, save_path):
        """
        Save the complete model for future use
        """
        print(f"\nüíæ Saving complete model...")
        
        try:
            model.save(save_path)
            
            # Save metadata
            metadata = {
                "model_type": "SRGAN_Generator_YourWeights",
                "input_size": [48, 48, 3],
                "output_size": [192, 192, 3],
                "scale_factor": 4,
                "architecture": "Custom SRGAN matching your weights structure",
                "weight_structure": "conv2d layers with residual blocks and upsampling",
                "paper_compliance": "4x upscaling (48x48 -> 192x192)"
            }
            
            metadata_path = save_path.replace('.h5', '_metadata.json')
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            print(f"‚úÖ Model saved to: {save_path}")
            print(f"üìÑ Metadata saved to: {metadata_path}")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to save model: {e}")
            return False

def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Load your specific SRGAN weights and create complete model"
    )
    parser.add_argument('weights_path', help='Path to your SRGAN weights .h5 file')
    parser.add_argument('--save-model', default='srgan_your_weights.h5',
                       help='Path to save complete model')
    parser.add_argument('--test-image', help='Test with specific image')
    
    args = parser.parse_args()
    
    print("üéØ SRGAN SPECIFIC WEIGHTS LOADER")
    print("Designed for your exact weights file structure")
    print("=" * 60)
    
    if not Path(args.weights_path).exists():
        print(f"‚ùå Weights file not found: {args.weights_path}")
        return
    
    # Initialize loader
    loader = SRGANSpecificLoader()
    
    try:
        # Create model from your weights
        model, load_method = loader.create_srgan_from_your_weights(args.weights_path)
        
        print(f"\n‚úÖ Model created successfully!")
        print(f"   Loading method: {load_method}")
        print(f"   Input shape: {model.input_shape}")
        print(f"   Output shape: {model.output_shape}")
        
        # Test functionality
        working_range = loader.test_model_functionality(model)
        
        # Save complete model
        if loader.save_complete_model(model, args.save_model):
            print(f"\nüéâ Success! Complete model ready for use.")
            print(f"\nüìã Next steps:")
            print(f"1. Use the complete model for preprocessing:")
            print(f"   python updated_complete_pipeline.py {args.save_model} /path/to/raf_db --test-mode")
            print(f"\n2. Or use it directly in preprocessing:")
            print(f"   python srgan_standard_preprocessing.py {args.save_model} /path/to/dataset /path/to/output")
        
        # Test with real image if provided
        if args.test_image and Path(args.test_image).exists() and working_range:
            print(f"\nüñºÔ∏è  Testing with real image...")
            
            image = cv2.imread(args.test_image)
            if image is not None:
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image_resized = cv2.resize(image_rgb, (48, 48))
                
                # Use best working range
                if "0-1" in working_range:
                    normalized = image_resized / 255.0
                elif "-1 to 1" in working_range:
                    normalized = (image_resized / 127.5) - 1.0
                else:
                    normalized = image_resized.astype(np.float32)
                
                # Process
                input_batch = np.expand_dims(normalized, axis=0)
                output = model.predict(input_batch, verbose=0)[0]
                
                # Denormalize
                if "0-1" in working_range:
                    output_denorm = np.clip(output * 255, 0, 255).astype(np.uint8)
                elif "-1 to 1" in working_range:
                    output_denorm = np.clip((output + 1) * 127.5, 0, 255).astype(np.uint8)
                else:
                    output_denorm = np.clip(output, 0, 255).astype(np.uint8)
                
                # Save comparison
                original_upscaled = cv2.resize(image_resized, (192, 192), interpolation=cv2.INTER_CUBIC)
                comparison = np.hstack([original_upscaled, output_denorm])
                comparison_bgr = cv2.cvtColor(comparison, cv2.COLOR_RGB2BGR)
                
                cv2.imwrite("test_your_srgan.jpg", comparison_bgr)
                print(f"‚úÖ Test result saved to: test_your_srgan.jpg")
        
    except Exception as e:
        print(f"‚ùå Failed to create model: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()